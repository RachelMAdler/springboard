{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h1>SUMMARY</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, org_id and invited_by_user_id are the most predictive of whether or not a user will adopt the product.  In particular, the organizations with the ids 395, 392, 345, 305, and 387 have the highest mean user adoption rate.\n",
    "\n",
    "**Summary of approach**\n",
    "\n",
    "The factors org_id, invited_by_user_id, creation_source, opted_in_to_mailing_list, and enabled_for_marketing_drip were entered as predictor variables in a logistic regression model predicting adoption (1 or 0).  All predictor variables were scaled to be between 0 and 1.  The data were split into training and test sets (70% and 30%, respectively), and 5-fold cross-validation was performed.  The class_weight parameter was adjusted to account for the class imbalance (lower rate of 1 [adopted] vs. 0 [not adopted]).  Overall, the model achieved an AUC of 0.59 and F1 of 0.28.\n",
    "\n",
    "(See code below for more details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:58:24.816192Z",
     "start_time": "2017-11-15T15:58:23.297199-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:58:28.465724Z",
     "start_time": "2017-11-15T15:58:24.818753-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engage = pd.read_csv('takehome_user_engagement.csv')\n",
    "chardet.detect(open('takehome_users.csv','rb').read())\n",
    "users = pd.read_csv('takehome_users.csv', encoding='ISO-8859-1', index_col='object_id')\n",
    "users.index.names = ['user_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify adopted users\n",
    "(Users who have logged into product on three separate days in at least one seven-day period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:58:30.869496Z",
     "start_time": "2017-11-15T15:58:30.789402-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert time_stamp column to datetime object\n",
    "engage['time_stamp'] = pd.to_datetime(engage['time_stamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:58:31.781737Z",
     "start_time": "2017-11-15T15:58:31.060390-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create date column from time_stamp column\n",
    "engage['date'] = engage['time_stamp'].dt.date\n",
    "\n",
    "# delete time_stamp and visited columns\n",
    "engage.drop(['visited', 'time_stamp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:58:31.973292Z",
     "start_time": "2017-11-15T15:58:31.960431-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adopt_function(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    Determine whether user logged in on three separate days in at least one seven day period\n",
    "    If so, return 1; if not, return 0\n",
    "    \"\"\"\n",
    "    \n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    # create variable to store result (whether user is/isn't adopted)\n",
    "    adopted = 0\n",
    "    \n",
    "    # convert inputted dates to list\n",
    "    x = list(x)\n",
    "\n",
    "    # sort user login dates\n",
    "    x.sort()\n",
    "\n",
    "    # create list to store time differences between each day and next day\n",
    "    tds = []\n",
    "\n",
    "    # calculate difference between current and next day and add to list 'tds'\n",
    "    for i in range(len(x)-2):\n",
    "        tds.append(x[i+1] - x[i])\n",
    "\n",
    "    # create list to store instances where user logged in 3 or more times in 7 days\n",
    "    logins = []\n",
    "    \n",
    "    # add up current day, day+1, and day+2\n",
    "    # if any sum is less than or equal to 7 days, return 1 (else, return 0)\n",
    "    for i in range(len(tds)-2):\n",
    "        tds_sum = (tds[i] + tds[i+1] + tds[i+2])\n",
    "        if tds_sum <= timedelta(days=7):\n",
    "            adopted = 1\n",
    "\n",
    "    return adopted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:58:34.318152Z",
     "start_time": "2017-11-15T15:58:33.602831-05:00"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for each user, determine whether they're considered \"adopted\"\n",
    "adopted = engage.groupby('user_id').agg(adopt_function)\n",
    "adopted.columns = ['adopted']\n",
    "\n",
    "# add adopted column to user df\n",
    "users = users.merge(adopted, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying predictive features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:58:45.500380Z",
     "start_time": "2017-11-15T15:58:45.436572-05:00"
    }
   },
   "outputs": [],
   "source": [
    "df_mod = users.copy()\n",
    "\n",
    "# remove rows with no entry for adopted\n",
    "df_mod = df_mod[pd.notnull(df_mod['adopted'])]\n",
    "\n",
    "# delete unnecessary columns (name, email)\n",
    "df_mod.drop(['name', 'email'], axis=1, inplace=True)\n",
    "\n",
    "# drop the 'creation_time' and 'last_session_creation_time' columns\n",
    "df_mod.drop(['creation_time', 'last_session_creation_time'], axis=1, inplace=True)\n",
    "\n",
    "# replace text columns with numeric representations\n",
    "creation_dict = {'ORG_INVITE': 1,\n",
    "                 'SIGNUP': 2,\n",
    "                 'GUEST_INVITE': 3,\n",
    "                 'SIGNUP_GOOGLE_AUTH': 4,\n",
    "                 'PERSONAL_PROJECTS': 5}\n",
    "\n",
    "df_mod['creation_source'] = df_mod['creation_source'].map(creation_dict)\n",
    "\n",
    "# drop rows with NaNs\n",
    "df_mod = df_mod.dropna() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X and y and scale values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:58:47.254322Z",
     "start_time": "2017-11-15T15:58:47.238861-05:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ,  0.        ,  0.02650602,  0.9003001 ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.00240964,  0.02609203],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.22650602,  0.12687563],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        ,  0.06024096,  0.32852618],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.21445783,  0.68856285],\n",
       "       [ 1.        ,  1.        ,  1.        ,  0.2       ,  0.6728076 ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create X and y\n",
    "y = df_mod['adopted']\n",
    "X_cols = [x for x in df_mod.columns if (x != 'adopted')]\n",
    "X = df_mod[X_cols]\n",
    "\n",
    "# scale values in dataframe to be between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:58:49.703514Z",
     "start_time": "2017-11-15T15:58:49.655607-05:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rachel/.pyenv/versions/py36_env/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "_, itest = train_test_split(range(df_mod.shape[0]), train_size = 0.7)\n",
    "mask = np.zeros(df_mod.shape[0], dtype=np.bool)\n",
    "mask[itest] = True\n",
    "\n",
    "test = df_mod[mask]\n",
    "train = df_mod[~mask]\n",
    "Xtest = test[X_cols]\n",
    "ytest = test['adopted']\n",
    "Xtrain = train[X_cols]\n",
    "ytrain = train['adopted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:58:50.822466Z",
     "start_time": "2017-11-15T15:58:50.814150-05:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of adopted (1) in training set: 15.70%\n",
      "Percent of adopted (1) in test set: 16.12%\n"
     ]
    }
   ],
   "source": [
    "# make sure proportion of classes in training and test sets is about equal\n",
    "prop_adopt = sum(train['adopted'])/len(train)\n",
    "print('Percent of adopted (1) in training set: {:.2f}%'.format(prop_adopt*100))\n",
    "\n",
    "prop_adopt = sum(test['adopted'])/len(test)\n",
    "print('Percent of adopted (1) in test set: {:.2f}%'.format(prop_adopt*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:58:51.714971Z",
     "start_time": "2017-11-15T15:58:51.682314-05:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==================================================================\n",
    "Perform cross-validation\n",
    "==================================================================\n",
    "\"\"\"\n",
    "\n",
    "def cv_optimize(clf, dict_params, Xtrain, ytrain, scorer, n_folds):\n",
    "    \n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    gs = GridSearchCV(clf,\n",
    "                      param_grid = dict_params,\n",
    "                      cv = n_folds,\n",
    "                      scoring = scorer) \n",
    "    \n",
    "    gs.fit(Xtrain, ytrain)\n",
    "    \n",
    "    print('Cross validation results:')\n",
    "    print('Best parameters: ' + str(gs.best_params_))\n",
    "    print('Best score: {:.2f}\\n'.format(gs.best_score_))\n",
    "    \n",
    "    return gs.best_estimator_, gs.best_params_\n",
    "\n",
    "\"\"\"\n",
    "==================================================================\n",
    "Evaluate model\n",
    "==================================================================\n",
    "\"\"\"\n",
    "\n",
    "def model_eval(clf, Xtrain, ytrain, Xtest, ytest):\n",
    "    \n",
    "    # get predictions for y\n",
    "    ypred_test = clf.predict(Xtest)\n",
    "    ypred_train = clf.predict(Xtrain)\n",
    "    ypred_proba = clf.predict_proba(Xtest)\n",
    "    auc = metrics.roc_auc_score(ytest, ypred_proba[:,1])\n",
    "\n",
    "    # create confusion matrix\n",
    "    cnf_matrix = metrics.confusion_matrix(ytest, ypred_test)\n",
    "    \n",
    "    # print model performance summary\n",
    "    print('Training accuracy: {:.2f}%'.format(100*metrics.accuracy_score(ypred_train, ytrain)),\n",
    "          '\\nTest accuracy: {:.2f}%'.format(100*metrics.accuracy_score(ypred_test, ytest)),\n",
    "          '\\nPrecision: {:.2f}'.format(metrics.precision_score(ytest, ypred_test)),\n",
    "          '\\nRecall: {:.2f}'.format(metrics.recall_score(ytest, ypred_test)),\n",
    "          '\\nF1: {:.2f}'.format(metrics.f1_score(ytest, ypred_test)),\n",
    "          '\\nAUC: {:.2f}'.format(auc),\n",
    "          '\\nTrue Positive Rate: {:.2f}'.format(float(cnf_matrix[0][0])/np.sum(cnf_matrix[0])),\n",
    "          '\\nTrue Negative Rate: {:.2f}'.format(float(cnf_matrix[1][1])/np.sum(cnf_matrix[1]))\n",
    "         )\n",
    "\n",
    "    print('Confusion matrix:')\n",
    "    print(cnf_matrix)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:58:52.853604Z",
     "start_time": "2017-11-15T15:58:52.490158-05:00"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation results:\n",
      "Best parameters: {'C': 0.001, 'class_weight': {0: 0.15, 1: 0.85}, 'fit_intercept': False}\n",
      "Best score: 0.50\n",
      "\n",
      "Training accuracy: 43.94% \n",
      "Test accuracy: 42.50% \n",
      "Precision: 0.18 \n",
      "Recall: 0.71 \n",
      "F1: 0.28 \n",
      "AUC: 0.56 \n",
      "True Positive Rate: 0.37 \n",
      "True Negative Rate: 0.71\n",
      "Confusion matrix:\n",
      "[[446 756]\n",
      " [ 68 163]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coeff_AV</th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>org_id</th>\n",
       "      <td>0.137914</td>\n",
       "      <td>0.137914</td>\n",
       "      <td>org_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invited_by_user_id</th>\n",
       "      <td>0.055387</td>\n",
       "      <td>-0.055387</td>\n",
       "      <td>invited_by_user_id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>creation_source</th>\n",
       "      <td>0.011249</td>\n",
       "      <td>0.011249</td>\n",
       "      <td>creation_source</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enabled_for_marketing_drip</th>\n",
       "      <td>0.001403</td>\n",
       "      <td>-0.001403</td>\n",
       "      <td>enabled_for_marketing_drip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>opted_in_to_mailing_list</th>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>opted_in_to_mailing_list</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Coeff_AV  Coefficient                     Feature\n",
       "org_id                      0.137914     0.137914                      org_id\n",
       "invited_by_user_id          0.055387    -0.055387          invited_by_user_id\n",
       "creation_source             0.011249     0.011249             creation_source\n",
       "enabled_for_marketing_drip  0.001403    -0.001403  enabled_for_marketing_drip\n",
       "opted_in_to_mailing_list    0.000475     0.000475    opted_in_to_mailing_list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "dict_params = {'C': [.001, .001, .01, .1, 1],\n",
    "               'class_weight': [{0:.15, 1:.85}], # to account for smaller proportion of adopted\n",
    "               'fit_intercept': [False]}\n",
    "\n",
    "scorer = metrics.make_scorer(metrics.auc, reorder=True)\n",
    "\n",
    "# perform 5-fold cross-validation\n",
    "clf_LR, best_params_LR = cv_optimize(clf, dict_params, Xtrain, ytrain, scorer, n_folds=5)\n",
    "\n",
    "# print model evaluation metrics\n",
    "model_eval(clf_LR, Xtrain, ytrain, Xtest, ytest)\n",
    "\n",
    "# print feature importances\n",
    "coefs = np.std(Xtest, 0)*clf_LR.coef_[0,0:]\n",
    "coef_df = pd.DataFrame({'Feature': X_cols,\n",
    "                        'Coefficient': coefs,\n",
    "                        'Coeff_AV': abs(coefs)})\n",
    "coef_df.sort_values(by = 'Coeff_AV', ascending = False, inplace=True)\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine top org_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-15T20:58:54.596216Z",
     "start_time": "2017-11-15T15:58:54.583626-05:00"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adopted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>org_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         adopted\n",
       "org_id          \n",
       "395     0.750000\n",
       "392     0.750000\n",
       "345     0.666667\n",
       "305     0.666667\n",
       "387     0.666667"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print top 5 organizations with the highest mean adoption rate\n",
    "df_temp = df_mod[['org_id','adopted']]\n",
    "df_temp = df_temp.groupby('org_id').mean()\n",
    "df_temp.sort_values('adopted', ascending=False).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
