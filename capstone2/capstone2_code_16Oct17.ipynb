{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h1>SETUP</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:01.254744Z",
     "start_time": "2017-10-16T19:15:00.636832-04:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:02.039393Z",
     "start_time": "2017-10-16T19:15:02.030717-04:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setup_input():\n",
    "\n",
    "    colnames = ['user', 'user_id', 'tweet_id', 'text', 'hashtags',\n",
    "                'tweet_date', 'user_start', 'followers_count',\n",
    "                'friends', 'total_tweets', 'location', 'timezone',\n",
    "                'user_tweet_id', 'sarcasm', 'sarcastic', 'irony',\n",
    "                'ironic', 'happy', 'sad', 'seriously']\n",
    "    \n",
    "    dtypes_file = {'user':str, 'user_id':str, 'tweet_id':str, 'text':str, 'hashtags':str,\n",
    "                'tweet_date':str, 'user_start':str, 'followers_count':str,\n",
    "                'friends':str, 'total_tweets':str, 'location':str, 'timezone':str,\n",
    "                'user_tweet_id':str, 'sarcasm':str, 'sarcastic':str, 'irony':str,\n",
    "                'ironic':str, 'happy':str, 'sad':str, 'seriously':str}\n",
    "    \n",
    "    return colnames, dtypes_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:02.407907Z",
     "start_time": "2017-10-16T19:15:02.380055-04:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_input(path, num_files, hashtags):\n",
    "    \n",
    "    get_time('Start time: ')\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    num_file = 0\n",
    "    \n",
    "    # Set up for getting input\n",
    "    colnames, dtypes_file = setup_input()\n",
    "\n",
    "    # Calculate and print total number of folders and files\n",
    "    _, files = get_num_folders_files(path)\n",
    "\n",
    "    # iterate through files (containing tweets) in folder:\n",
    "    for i in next(os.walk(path))[2]:\n",
    "\n",
    "        if not i.startswith('hashtag'):\n",
    "            continue\n",
    "\n",
    "        num_file += 1\n",
    "        \n",
    "        if num_file >= num_files:\n",
    "            break\n",
    "\n",
    "        print_str = 'Importing file: ' + str(i) + ' (#' + str(num_file) + ' of ' + str(files) + '), time: '\n",
    "        get_time(print_str)\n",
    "\n",
    "        fullpath = os.path.join(str(path), str(i))\n",
    "        \n",
    "        df_temp = pd.read_csv(filepath_or_buffer = fullpath, sep = '\\t',\n",
    "                                  names=colnames, error_bad_lines=True,\n",
    "                                  dtype=dtypes_file, encoding='utf-8')\n",
    "        \n",
    "        # clean file\n",
    "        df_temp = clean_file(df_temp, hashtags)\n",
    "\n",
    "        # if first csv in group, save as df:   \n",
    "        if num_file == 1:\n",
    "            df = df_temp\n",
    "        else:\n",
    "            df = df.append(df_temp)\n",
    "                \n",
    "    get_time('\\nDone! End time: ')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:02.726212Z",
     "start_time": "2017-10-16T19:15:02.705944-04:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_in_dfs(df_list, hashtags):\n",
    "    \n",
    "    colnames = ['user_tweet_id', 'user', 'user_id', 'tweet_id', 'text', 'hashtags',\n",
    "                'tweet_date', 'user_start', 'followers_count',\n",
    "                'friends', 'total_tweets', 'location', 'timezone',\n",
    "                'sarcasm', 'sarcastic', 'irony', 'ironic', 'happy',\n",
    "                'sad', 'seriously', 'num_ht']\n",
    "    \n",
    "    dtypes_file = {'user_tweet_id':str, 'user':str, 'user_id':str, 'tweet_id':str, 'text':str, 'hashtags':str,\n",
    "                'tweet_date':str, 'user_start':str, 'followers_count':str,\n",
    "                'friends':str, 'total_tweets':str, 'location':str, 'timezone':str,\n",
    "                'sarcasm':str, 'sarcastic':str, 'irony':str, 'ironic':str, 'happy':str,\n",
    "                'sad':str, 'seriously':str, 'num_ht': str}\n",
    "    \n",
    "    dict_dfs = {}\n",
    "\n",
    "    for i in range(0, len(df_list)):\n",
    "\n",
    "        key = df_list[i]\n",
    "        path = './hashtag_dfs/' + df_list[i] + '.txt'\n",
    "        df = pd.read_csv(filepath_or_buffer = path, sep = '\\t',\n",
    "                         names = colnames, error_bad_lines = True,\n",
    "                         dtype=dtypes_file, encoding = 'utf-8')\n",
    "        \n",
    "        df = clean_file(df, hashtags)\n",
    "        \n",
    "        value = df\n",
    "        dict_dfs[key] = value \n",
    "        \n",
    "        print('Dataframe ' + df_list[i] + ' loaded, length = {:,}.'.format(len(df)))\n",
    "\n",
    "    return dict_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:03.055323Z",
     "start_time": "2017-10-16T19:15:03.045375-04:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_file(df, hashtags):\n",
    "    \n",
    "    # Remove first row (duplicate with column names)\n",
    "    df = df.iloc[1:]\n",
    "    \n",
    "    # Convert numeric columns to integers:\n",
    "    pd.to_numeric(df['followers_count'], errors='coerce')\n",
    "    pd.to_numeric(df['friends'], errors='coerce')\n",
    "    pd.to_numeric(df['total_tweets'], errors='coerce')\n",
    "    \n",
    "    df['followers_count'] = df['followers_count'].astype(int)\n",
    "    df['friends'] = df['friends'].astype(int)\n",
    "    df['total_tweets'] = df['total_tweets'].astype(int)\n",
    "    \n",
    "    for ht in hashtags:\n",
    "        pd.to_numeric(df[ht], errors ='coerce')\n",
    "        df[ht] = df[ht].astype(int)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get number of folders and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:03.365843Z",
     "start_time": "2017-10-16T19:15:03.359457-04:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_folders_files(path):\n",
    "    \n",
    "    folders = 0\n",
    "    files = 0\n",
    "\n",
    "    for _, dirnames, filenames in os.walk(path):\n",
    "        folders += len(dirnames)\n",
    "        files += len(filenames)\n",
    "    \n",
    "    print('\\nThere are {:,} total folders and {:,} total files.'.format(folders, files))\n",
    "    \n",
    "    return folders, files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:03.717058Z",
     "start_time": "2017-10-16T19:15:03.708506-04:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_time(print_string):\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    hour=int(now.hour)\n",
    "    minute=int(now.minute)\n",
    "    second=int(now.second)\n",
    "    \n",
    "    if(len(str(abs(minute)))) == 1:\n",
    "        minute_str = \"0\" + str(minute)\n",
    "    else:\n",
    "        minute_str = str(minute)    \n",
    "    \n",
    "    if print_string != \"\":\n",
    "        print(print_string + str(hour) + \":\" + str(minute_str) + \":\" + str(second))\n",
    "    else:\n",
    "        return hour, minute_str, second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h1>DATA CLEANING</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "Drop rows with missing tweet text and report total number of tweets and users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:04.256365Z",
     "start_time": "2017-10-16T19:15:04.247871-04:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \n",
    "    # drop rows with missing tweet text\n",
    "    df = df[~df.text.isnull()]\n",
    "    \n",
    "    # if index hasn't been set already\n",
    "    if df.index.name == None:\n",
    "        # remove duplicate tweets\n",
    "        df = df.drop_duplicates(subset='user_tweet_id', keep = 'first')\n",
    "        # set index to 'user_tweet_id' column\n",
    "        df = df.set_index(['user_tweet_id'])\n",
    "    \n",
    "    # encode tweet text as utf-8\n",
    "    col_text = df.columns.get_loc('text')\n",
    "    for i in range(0,len(df)):\n",
    "        df.iloc[i,col_text].encode('utf-8')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count total hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:04.635358Z",
     "start_time": "2017-10-16T19:15:04.631222-04:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_total_ht(df):\n",
    "    \n",
    "    df['num_ht'] = 0\n",
    "    df['num_ht'][df['hashtags'] != '[]'] = df.hashtags.str.count(',')+1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:05.003412Z",
     "start_time": "2017-10-16T19:15:04.994923-04:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_ht(df, perform_subset, ht):\n",
    "\n",
    "    if perform_subset == True:\n",
    "        \n",
    "        df = df[df[ht] == 1]\n",
    "    \n",
    "    n_tweets = len(df)\n",
    "    n_users = df.user.unique().size\n",
    "\n",
    "    print('\\nNumber of tweets with {} hashtag: {:,d}'.format(ht, n_tweets))\n",
    "    print('Number of users with {} hashtag: {:,d}'.format(ht, n_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h1>TEST-TRAIN SPLIT</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:05.949315Z",
     "start_time": "2017-10-16T19:15:05.359297-04:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rachel/.pyenv/versions/py36_env/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def create_train_test(df, filename_base):\n",
    "    \n",
    "    tt_exist = False\n",
    "\n",
    "    # set file names\n",
    "    mask_file = './training_test_sets/mask_' + filename_base + '.txt'\n",
    "\n",
    "    \"\"\"If training and test sets exist, load them; otherwise, create them\"\"\"\n",
    "    if os.path.isfile(mask_file):\n",
    "        \n",
    "        tt_exist = True\n",
    "        \n",
    "        print('\\nTraining and test set files already exist, opening now.')\n",
    "        \n",
    "        print('Reading in mask from ' + mask_file)\n",
    "        mask = np.loadtxt(mask_file, delimiter = '\\t')\n",
    "        #mask = mask[1:] # remove header row\n",
    "        \n",
    "        mask = mask.astype(bool)\n",
    "        \n",
    "        train_data = df[~mask]\n",
    "        test_data = df[mask]\n",
    "\n",
    "    else: \n",
    "\n",
    "        print('\\nTraining and test set files do not exist, generating now.')\n",
    "\n",
    "        # create training and test sets\n",
    "        # in below, itest is an array containing the row numbers of the test set (which is 30% of the overall dataset)\n",
    "        _, itest = train_test_split(range(df.shape[0]), train_size = 0.7)\n",
    "        mask = np.zeros(df.shape[0], dtype=np.bool)\n",
    "        mask[itest] = True\n",
    "\n",
    "        train_data = df[~mask]\n",
    "        test_data = df[mask]\n",
    "            \n",
    "        print('Saving mask to ' + mask_file)\n",
    "        np.savetxt(fname = mask_file, X = mask, delimiter = '\\t')\n",
    "        \n",
    "    print('\\nLength of test: {:,}'.format(len(test_data)))\n",
    "    print('Length of train: {:,}'.format(len(train_data)))\n",
    "\n",
    "    return train_data, test_data, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h1>MODELING</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:09.673300Z",
     "start_time": "2017-10-16T19:15:09.658453-04:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def freq_words(num_words, cv, X, top):\n",
    "\n",
    "    words = np.array(cv.get_feature_names())\n",
    "\n",
    "    # calculate word frequencies\n",
    "    freq = np.asarray(X.sum(axis=0)).ravel()\n",
    "\n",
    "    # sort rows by frequencies (low to high) and return indices:\n",
    "    ind = np.argsort(freq)\n",
    "\n",
    "    print('\\nTotal # of words in vocabulary: {:,}'.format(len(words)))\n",
    "    \n",
    "    if top == True:\n",
    "        # freq_words = words with highest probability\n",
    "        # below, freq_words represents N words from end (highest freqs)\n",
    "        freq_words = words[ind[-num_words:]]\n",
    "        freq_num = freq[ind[-num_words:]]\n",
    "        \n",
    "        print('Most frequent words\\n')\n",
    "        for w, p in zip(freq_words, freq_num):\n",
    "            print('{:>5}'.format(p), '{:>10}'.format(w))\n",
    "    else:\n",
    "        # freq_words = words with highest probability\n",
    "        freq_words = words[ind[:num_words]]\n",
    "        freq_num = freq[ind[:num_words]]\n",
    "        print('Least frequent words\\n')\n",
    "        for w, p in zip(freq_words, freq_num):\n",
    "            print('{:>5}'.format(p), '{:>10}'.format(w))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine most predictive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:10.620528Z",
     "start_time": "2017-10-16T19:15:10.603891-04:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictive_words(num_words, cv, clf, Xtest):\n",
    "\n",
    "    words = np.array(cv.get_feature_names())\n",
    "    \n",
    "    # create identity matrix (each row has exactly 1 feature):\n",
    "    x = np.eye(Xtest.shape[1]) \n",
    "\n",
    "    # use trained classifier to make predictions on this matrix:\n",
    "    probs = clf.predict_log_proba(x)[:, 0] \n",
    "\n",
    "    # sort rows by predicted probabilities and return indices:\n",
    "    ind = np.argsort(probs) \n",
    "\n",
    "    # good_words = words with highest probability\n",
    "    good_words = words[ind[:num_words]]\n",
    "\n",
    "    # bad_words = words with lowest probability\n",
    "    bad_words = words[ind[-num_words:]]\n",
    "\n",
    "    good_prob = probs[ind[:num_words]]\n",
    "    bad_prob = probs[ind[-num_words:]]\n",
    "\n",
    "    print('\\nMost predictive words')\n",
    "    print('\\tP(sarcasm | word)')\n",
    "    for w, p in zip(good_words, good_prob):\n",
    "        print('{:>15}'.format(w), '{:.2f}'.format(1 - np.exp(p)))\n",
    "\n",
    "    print('\\nLeast predictive words')\n",
    "    print('\\tP(sarcasm | word)')\n",
    "    for w, p in zip(bad_words, bad_prob):\n",
    "        print('{:>15}'.format(w), '{:.2f}'.format(1 - np.exp(p))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:53:41.721555Z",
     "start_time": "2017-10-16T19:53:41.694332-04:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def model_eval(clf, Xtrain, Xtest, ytrain, ytest):\n",
    "\n",
    "    ypred_test = clf.predict(Xtest)\n",
    "    ypred_train = clf.predict(Xtrain)\n",
    "    ypred_proba = clf.predict_proba(Xtest)\n",
    "\n",
    "    # accuracy\n",
    "    acc_train = round(100*metrics.accuracy_score(ypred_train, ytrain),2)\n",
    "    print('\\nAccuracy for training set = {:.2f}%'.format(acc_train))\n",
    "\n",
    "    acc_test = round(100*metrics.accuracy_score(ypred_test, ytest),2)\n",
    "    print('Accuracy for test set = {:.2f}%'.format(acc_test))\n",
    "\n",
    "    # cohen's kappa\n",
    "    cohenk = metrics.cohen_kappa_score(ytest, ypred_test)\n",
    "    print('Cohen\\'s Kappa = {:.2f}'.format(cohenk))\n",
    "    \n",
    "    # precision (ability of classifier not to label as positive a sample that is negative)\n",
    "    precision = metrics.precision_score(ytest, ypred_test)\n",
    "    print('Precision = {:.2f}'.format(precision))\n",
    "\n",
    "    # AUC\n",
    "    auc = metrics.roc_auc_score(ytest, ypred_proba[:,1])\n",
    "    print('AUC = {:.2f}'.format(auc))\n",
    "    \n",
    "    # create confusion matrix\n",
    "    classes = np.unique(ytest)\n",
    "\n",
    "    cnf_matrix = metrics.confusion_matrix(ytest, ypred_test)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    tp = float(cnf_matrix[0][0])/np.sum(cnf_matrix[0])\n",
    "    tn = float(cnf_matrix[1][1])/np.sum(cnf_matrix[1])\n",
    "    \n",
    "    print('True positive rate = {:.2f}'.format(tp))\n",
    "    print('True negative rate = {:.2f}'.format(tn))\n",
    "\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes, False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:13.011983Z",
     "start_time": "2017-10-16T19:15:12.786500-04:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize, cmap=plt.cm.Blues):\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        title = 'Confusion matrix, with normalization'\n",
    "    else:\n",
    "        title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:39:58.749689Z",
     "start_time": "2017-10-16T19:39:58.741608-04:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cv_score(clf, X, y, scorefunc):\n",
    "    result = 0.\n",
    "    nfold = 5\n",
    "    \n",
    "    # split data into train/test groups, 5 times\n",
    "    for train, test in KFold(nfold, shuffle=True).split(X, y):\n",
    "\n",
    "        # fit the classifier, passed is as clf\n",
    "        clf.fit(X[train], y[train]) \n",
    "\n",
    "        # evaluate score function on held-out data\n",
    "        result += scorefunc(clf, X[test], y[test])\n",
    "        \n",
    "    # average score\n",
    "    return result / nfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:32:51.950185Z",
     "start_time": "2017-10-16T19:32:51.945752-04:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auc(clf, x, y):\n",
    "    \n",
    "    pred = clf.predict_proba(x) \n",
    "    \n",
    "    return roc_auc_score(y, pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:16.998454Z",
     "start_time": "2017-10-16T19:15:16.994056-04:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def cohenk(clf, x, y):\n",
    "    \n",
    "    pred = clf.predict(x)\n",
    "    \n",
    "    return cohen_kappa_score(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:17.315749Z",
     "start_time": "2017-10-16T19:15:17.310381-04:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(clf, x, y):\n",
    "    \n",
    "    pred = clf.predict_log_proba(x)\n",
    "    sarcasm = y == 1\n",
    "    not_sarcasm = ~sarcasm\n",
    "    \n",
    "    # add together:\n",
    "    # 1. the log probability of sarcasm samples being labeled 1 (sarcasm)\n",
    "    # 2. the log probability of non-sarcasm samples being labeled 0 (non-sarcasm)\n",
    "    return pred[sarcasm, 1].sum() + pred[not_sarcasm, 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:54:17.325616Z",
     "start_time": "2017-10-16T19:54:17.300016-04:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def perform_cv(df, mask, alphas, min_dfs, max_dfs, score_func):\n",
    "    \n",
    "    #Find the best value for alpha and min_df, and the best classifier\n",
    "    max_score=-np.inf\n",
    "    best_alpha=-np.inf\n",
    "    best_min_df=-np.inf\n",
    "    best_max_df=-np.inf\n",
    "    \n",
    "    #mask = mask.astype(int)\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        for min_df in min_dfs:\n",
    "            for max_df in max_dfs:\n",
    "                vectorizer = CountVectorizer(min_df=min_df, max_df=max_df, stop_words = 'english')       \n",
    "                Xthis, ythis, _ = make_xy(df, vectorizer)\n",
    "\n",
    "                if Xthis == None:\n",
    "                    print('After pruning, no terms remain; alpha: {}, min_df: {}, max_df: {}'.\n",
    "                          format(alpha, min_df, max_df))\n",
    "\n",
    "                    continue\n",
    "                else:\n",
    "                    # only use training data for cross-validation\n",
    "                    Xtrainthis = Xthis[~mask]\n",
    "                    ytrainthis = ythis[~mask]\n",
    "\n",
    "                    clf = MultinomialNB(alpha = alpha)\n",
    "                    \n",
    "                    score = cv_score(clf, Xtrainthis, ytrainthis, score_func)\n",
    "\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "                        best_alpha = alpha\n",
    "                        best_min_df = min_df\n",
    "                        best_max_df = max_df\n",
    "\n",
    "    print('Score: {:.3}, alpha: {}, min_df: {}, max_df: {}'.format(max_score, best_alpha, best_min_df, best_max_df))\n",
    "    \n",
    "    return best_alpha, best_min_df, best_max_df, clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:15:25.078251Z",
     "start_time": "2017-10-16T19:15:25.060497-04:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def make_xy(df, vectorizer):\n",
    "    \n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer()\n",
    "    \n",
    "    # call `fit` to build the vocabulary\n",
    "    try:\n",
    "        vectorizer.fit(df['text'])\n",
    "\n",
    "        # call `transform` to convert text to a bag of words\n",
    "        x = vectorizer.transform(df['text'])\n",
    "\n",
    "        X = x.tocsc()  # some versions of sklearn return COO format\n",
    "\n",
    "        # CountVectorizer uses a sparse array to save memory, but it's easier in this assignment to \n",
    "        # convert back to a \"normal\" numpy array\n",
    "        #X = X.toarray()\n",
    "\n",
    "        # convert sarcasm column to integers\n",
    "        y = (df['sarcasm'] == 1).values.astype(np.int) \n",
    "\n",
    "    except ValueError:\n",
    "    \n",
    "        X = None\n",
    "        y = None\n",
    "        vectorizer = None\n",
    "\n",
    "    return X, y, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h1>MAIN CODE</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:19:38.388868Z",
     "start_time": "2017-10-16T19:15:38.358463-04:00"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes do not exist, creating now.\n",
      "Start time: 19:15:38\n",
      "\n",
      "There are 0 total folders and 367 total files.\n",
      "Importing file: hashtag_file0_folder11to15_14Oct17.txt (#1 of 367), time: 19:15:38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rachel/.pyenv/versions/py36_env/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/Rachel/.pyenv/versions/py36_env/lib/python3.6/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/Users/Rachel/.pyenv/versions/py36_env/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/Users/Rachel/.pyenv/versions/py36_env/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing file: hashtag_file0_folder16to20_14Oct17.txt (#2 of 367), time: 19:15:38\n",
      "Importing file: hashtag_file0_folder1to10_14Oct17.txt (#3 of 367), time: 19:15:39\n",
      "Importing file: hashtag_file0_folder21to28_14Oct17.txt (#4 of 367), time: 19:15:39\n",
      "Importing file: hashtag_file100_folder1to10_14Oct17.txt (#5 of 367), time: 19:15:40\n",
      "Importing file: hashtag_file100_folder21to28_14Oct17.txt (#6 of 367), time: 19:15:40\n",
      "Importing file: hashtag_file101_folder1to10_14Oct17.txt (#7 of 367), time: 19:15:41\n",
      "Importing file: hashtag_file101_folder21to28_14Oct17.txt (#8 of 367), time: 19:15:41\n",
      "Importing file: hashtag_file102_folder1to10_14Oct17.txt (#9 of 367), time: 19:15:42\n",
      "Importing file: hashtag_file102_folder21to28_14Oct17.txt (#10 of 367), time: 19:15:42\n",
      "Importing file: hashtag_file103_folder1to10_14Oct17.txt (#11 of 367), time: 19:15:43\n",
      "Importing file: hashtag_file103_folder21to28_14Oct17.txt (#12 of 367), time: 19:15:43\n",
      "Importing file: hashtag_file104_folder1to10_14Oct17.txt (#13 of 367), time: 19:15:44\n",
      "Importing file: hashtag_file104_folder21to28_14Oct17.txt (#14 of 367), time: 19:15:44\n",
      "Importing file: hashtag_file105_folder1to10_14Oct17.txt (#15 of 367), time: 19:15:45\n",
      "Importing file: hashtag_file105_folder21to28_14Oct17.txt (#16 of 367), time: 19:15:45\n",
      "Importing file: hashtag_file106_folder1to10_14Oct17.txt (#17 of 367), time: 19:15:46\n",
      "Importing file: hashtag_file106_folder21to28_14Oct17.txt (#18 of 367), time: 19:15:46\n",
      "Importing file: hashtag_file107_folder1to10_14Oct17.txt (#19 of 367), time: 19:15:47\n",
      "Importing file: hashtag_file107_folder21to28_14Oct17.txt (#20 of 367), time: 19:15:47\n",
      "Importing file: hashtag_file108_folder1to10_14Oct17.txt (#21 of 367), time: 19:15:48\n",
      "Importing file: hashtag_file108_folder21to28_14Oct17.txt (#22 of 367), time: 19:15:48\n",
      "Importing file: hashtag_file109_folder1to10_14Oct17.txt (#23 of 367), time: 19:15:48\n",
      "Importing file: hashtag_file109_folder21to28_14Oct17.txt (#24 of 367), time: 19:15:49\n",
      "Importing file: hashtag_file10_folder11to15_14Oct17.txt (#25 of 367), time: 19:15:49\n",
      "Importing file: hashtag_file10_folder16to20_14Oct17.txt (#26 of 367), time: 19:15:50\n",
      "Importing file: hashtag_file10_folder1to10_14Oct17.txt (#27 of 367), time: 19:15:50\n",
      "Importing file: hashtag_file10_folder21to28_14Oct17.txt (#28 of 367), time: 19:15:51\n",
      "Importing file: hashtag_file110_folder1to10_14Oct17.txt (#29 of 367), time: 19:15:51\n",
      "Importing file: hashtag_file110_folder21to28_14Oct17.txt (#30 of 367), time: 19:15:52\n",
      "Importing file: hashtag_file111_folder1to10_14Oct17.txt (#31 of 367), time: 19:15:52\n",
      "Importing file: hashtag_file111_folder21to28_14Oct17.txt (#32 of 367), time: 19:15:53\n",
      "Importing file: hashtag_file112_folder1to10_14Oct17.txt (#33 of 367), time: 19:15:53\n",
      "Importing file: hashtag_file112_folder21to28_14Oct17.txt (#34 of 367), time: 19:15:54\n",
      "Importing file: hashtag_file113_folder1to10_14Oct17.txt (#35 of 367), time: 19:15:54\n",
      "Importing file: hashtag_file113_folder21to28_14Oct17.txt (#36 of 367), time: 19:15:55\n",
      "Importing file: hashtag_file114_folder1to10_14Oct17.txt (#37 of 367), time: 19:15:55\n",
      "Importing file: hashtag_file114_folder21to28_14Oct17.txt (#38 of 367), time: 19:15:56\n",
      "Importing file: hashtag_file115_folder1to10_14Oct17.txt (#39 of 367), time: 19:15:56\n",
      "Importing file: hashtag_file115_folder21to28_14Oct17.txt (#40 of 367), time: 19:15:56\n",
      "Importing file: hashtag_file116_folder1to10_14Oct17.txt (#41 of 367), time: 19:15:57\n",
      "Importing file: hashtag_file116_folder21to28_14Oct17.txt (#42 of 367), time: 19:15:57\n",
      "Importing file: hashtag_file117_folder1to10_14Oct17.txt (#43 of 367), time: 19:15:58\n",
      "Importing file: hashtag_file117_folder21to28_14Oct17.txt (#44 of 367), time: 19:15:58\n",
      "Importing file: hashtag_file118_folder1to10_14Oct17.txt (#45 of 367), time: 19:15:59\n",
      "Importing file: hashtag_file118_folder21to28_14Oct17.txt (#46 of 367), time: 19:15:59\n",
      "Importing file: hashtag_file119_folder1to10_14Oct17.txt (#47 of 367), time: 19:16:0\n",
      "Importing file: hashtag_file119_folder21to28_14Oct17.txt (#48 of 367), time: 19:16:0\n",
      "Importing file: hashtag_file11_folder11to15_14Oct17.txt (#49 of 367), time: 19:16:1\n",
      "Importing file: hashtag_file11_folder16to20_14Oct17.txt (#50 of 367), time: 19:16:1\n",
      "Importing file: hashtag_file11_folder1to10_14Oct17.txt (#51 of 367), time: 19:16:2\n",
      "Importing file: hashtag_file11_folder21to28_14Oct17.txt (#52 of 367), time: 19:16:2\n",
      "Importing file: hashtag_file120_folder1to10_14Oct17.txt (#53 of 367), time: 19:16:3\n",
      "Importing file: hashtag_file120_folder21to28_14Oct17.txt (#54 of 367), time: 19:16:3\n",
      "Importing file: hashtag_file121_folder1to10_14Oct17.txt (#55 of 367), time: 19:16:4\n",
      "Importing file: hashtag_file121_folder21to28_14Oct17.txt (#56 of 367), time: 19:16:4\n",
      "Importing file: hashtag_file122_folder1to10_14Oct17.txt (#57 of 367), time: 19:16:5\n",
      "Importing file: hashtag_file122_folder21to28_14Oct17.txt (#58 of 367), time: 19:16:5\n",
      "Importing file: hashtag_file123_folder21to28_14Oct17.txt (#59 of 367), time: 19:16:6\n",
      "Importing file: hashtag_file124_folder21to28_14Oct17.txt (#60 of 367), time: 19:16:6\n",
      "Importing file: hashtag_file125_folder21to28_14Oct17.txt (#61 of 367), time: 19:16:7\n",
      "Importing file: hashtag_file12_folder11to15_14Oct17.txt (#62 of 367), time: 19:16:7\n",
      "Importing file: hashtag_file12_folder16to20_14Oct17.txt (#63 of 367), time: 19:16:8\n",
      "Importing file: hashtag_file12_folder1to10_14Oct17.txt (#64 of 367), time: 19:16:8\n",
      "Importing file: hashtag_file12_folder21to28_14Oct17.txt (#65 of 367), time: 19:16:9\n",
      "Importing file: hashtag_file13_folder11to15_14Oct17.txt (#66 of 367), time: 19:16:9\n",
      "Importing file: hashtag_file13_folder16to20_14Oct17.txt (#67 of 367), time: 19:16:10\n",
      "Importing file: hashtag_file13_folder1to10_14Oct17.txt (#68 of 367), time: 19:16:10\n",
      "Importing file: hashtag_file13_folder21to28_14Oct17.txt (#69 of 367), time: 19:16:11\n",
      "Importing file: hashtag_file14_folder11to15_14Oct17.txt (#70 of 367), time: 19:16:11\n",
      "Importing file: hashtag_file14_folder16to20_14Oct17.txt (#71 of 367), time: 19:16:12\n",
      "Importing file: hashtag_file14_folder1to10_14Oct17.txt (#72 of 367), time: 19:16:12\n",
      "Importing file: hashtag_file14_folder21to28_14Oct17.txt (#73 of 367), time: 19:16:13\n",
      "Importing file: hashtag_file15_folder11to15_14Oct17.txt (#74 of 367), time: 19:16:13\n",
      "Importing file: hashtag_file15_folder16to20_14Oct17.txt (#75 of 367), time: 19:16:14\n",
      "Importing file: hashtag_file15_folder1to10_14Oct17.txt (#76 of 367), time: 19:16:15\n",
      "Importing file: hashtag_file15_folder21to28_14Oct17.txt (#77 of 367), time: 19:16:15\n",
      "Importing file: hashtag_file16_folder11to15_14Oct17.txt (#78 of 367), time: 19:16:16\n",
      "Importing file: hashtag_file16_folder16to20_14Oct17.txt (#79 of 367), time: 19:16:16\n",
      "Importing file: hashtag_file16_folder1to10_14Oct17.txt (#80 of 367), time: 19:16:17\n",
      "Importing file: hashtag_file16_folder21to28_14Oct17.txt (#81 of 367), time: 19:16:17\n",
      "Importing file: hashtag_file17_folder11to15_14Oct17.txt (#82 of 367), time: 19:16:18\n",
      "Importing file: hashtag_file17_folder16to20_14Oct17.txt (#83 of 367), time: 19:16:18\n",
      "Importing file: hashtag_file17_folder1to10_14Oct17.txt (#84 of 367), time: 19:16:19\n",
      "Importing file: hashtag_file17_folder21to28_14Oct17.txt (#85 of 367), time: 19:16:19\n",
      "Importing file: hashtag_file18_folder11to15_14Oct17.txt (#86 of 367), time: 19:16:20\n",
      "Importing file: hashtag_file18_folder16to20_14Oct17.txt (#87 of 367), time: 19:16:20\n",
      "Importing file: hashtag_file18_folder1to10_14Oct17.txt (#88 of 367), time: 19:16:21\n",
      "Importing file: hashtag_file18_folder21to28_14Oct17.txt (#89 of 367), time: 19:16:21\n",
      "Importing file: hashtag_file19_folder11to15_14Oct17.txt (#90 of 367), time: 19:16:22\n",
      "Importing file: hashtag_file19_folder16to20_14Oct17.txt (#91 of 367), time: 19:16:22\n",
      "Importing file: hashtag_file19_folder1to10_14Oct17.txt (#92 of 367), time: 19:16:23\n",
      "Importing file: hashtag_file19_folder21to28_14Oct17.txt (#93 of 367), time: 19:16:24\n",
      "Importing file: hashtag_file1_folder11to15_14Oct17.txt (#94 of 367), time: 19:16:24\n",
      "Importing file: hashtag_file1_folder16to20_14Oct17.txt (#95 of 367), time: 19:16:25\n",
      "Importing file: hashtag_file1_folder1to10_14Oct17.txt (#96 of 367), time: 19:16:25\n",
      "Importing file: hashtag_file1_folder21to28_14Oct17.txt (#97 of 367), time: 19:16:26\n",
      "Importing file: hashtag_file20_folder11to15_14Oct17.txt (#98 of 367), time: 19:16:26\n",
      "Importing file: hashtag_file20_folder16to20_14Oct17.txt (#99 of 367), time: 19:16:27\n",
      "Importing file: hashtag_file20_folder1to10_14Oct17.txt (#100 of 367), time: 19:16:27\n",
      "Importing file: hashtag_file20_folder21to28_14Oct17.txt (#101 of 367), time: 19:16:28\n",
      "Importing file: hashtag_file21_folder11to15_14Oct17.txt (#102 of 367), time: 19:16:28\n",
      "Importing file: hashtag_file21_folder16to20_14Oct17.txt (#103 of 367), time: 19:16:29\n",
      "Importing file: hashtag_file21_folder1to10_14Oct17.txt (#104 of 367), time: 19:16:30\n",
      "Importing file: hashtag_file21_folder21to28_14Oct17.txt (#105 of 367), time: 19:16:30\n",
      "Importing file: hashtag_file22_folder11to15_14Oct17.txt (#106 of 367), time: 19:16:31\n",
      "Importing file: hashtag_file22_folder16to20_14Oct17.txt (#107 of 367), time: 19:16:31\n",
      "Importing file: hashtag_file22_folder1to10_14Oct17.txt (#108 of 367), time: 19:16:32\n",
      "Importing file: hashtag_file22_folder21to28_14Oct17.txt (#109 of 367), time: 19:16:32\n",
      "Importing file: hashtag_file23_folder11to15_14Oct17.txt (#110 of 367), time: 19:16:33\n",
      "Importing file: hashtag_file23_folder16to20_14Oct17.txt (#111 of 367), time: 19:16:33\n",
      "Importing file: hashtag_file23_folder1to10_14Oct17.txt (#112 of 367), time: 19:16:34\n",
      "Importing file: hashtag_file23_folder21to28_14Oct17.txt (#113 of 367), time: 19:16:35\n",
      "Importing file: hashtag_file24_folder11to15_14Oct17.txt (#114 of 367), time: 19:16:35\n",
      "Importing file: hashtag_file24_folder16to20_14Oct17.txt (#115 of 367), time: 19:16:36\n",
      "Importing file: hashtag_file24_folder1to10_14Oct17.txt (#116 of 367), time: 19:16:36\n",
      "Importing file: hashtag_file24_folder21to28_14Oct17.txt (#117 of 367), time: 19:16:37\n",
      "Importing file: hashtag_file25_folder11to15_14Oct17.txt (#118 of 367), time: 19:16:37\n",
      "Importing file: hashtag_file25_folder16to20_14Oct17.txt (#119 of 367), time: 19:16:38\n",
      "Importing file: hashtag_file25_folder1to10_14Oct17.txt (#120 of 367), time: 19:16:39\n",
      "Importing file: hashtag_file25_folder21to28_14Oct17.txt (#121 of 367), time: 19:16:39\n",
      "Importing file: hashtag_file26_folder11to15_14Oct17.txt (#122 of 367), time: 19:16:40\n",
      "Importing file: hashtag_file26_folder16to20_14Oct17.txt (#123 of 367), time: 19:16:40\n",
      "Importing file: hashtag_file26_folder1to10_14Oct17.txt (#124 of 367), time: 19:16:41\n",
      "Importing file: hashtag_file26_folder21to28_14Oct17.txt (#125 of 367), time: 19:16:41\n",
      "Importing file: hashtag_file27_folder11to15_14Oct17.txt (#126 of 367), time: 19:16:42\n",
      "Importing file: hashtag_file27_folder16to20_14Oct17.txt (#127 of 367), time: 19:16:43\n",
      "Importing file: hashtag_file27_folder1to10_14Oct17.txt (#128 of 367), time: 19:16:43\n",
      "Importing file: hashtag_file27_folder21to28_14Oct17.txt (#129 of 367), time: 19:16:44\n",
      "Importing file: hashtag_file28_folder11to15_14Oct17.txt (#130 of 367), time: 19:16:44\n",
      "Importing file: hashtag_file28_folder16to20_14Oct17.txt (#131 of 367), time: 19:16:45\n",
      "Importing file: hashtag_file28_folder1to10_14Oct17.txt (#132 of 367), time: 19:16:45\n",
      "Importing file: hashtag_file28_folder21to28_14Oct17.txt (#133 of 367), time: 19:16:46\n",
      "Importing file: hashtag_file29_folder11to15_14Oct17.txt (#134 of 367), time: 19:16:47\n",
      "Importing file: hashtag_file29_folder16to20_14Oct17.txt (#135 of 367), time: 19:16:47\n",
      "Importing file: hashtag_file29_folder1to10_14Oct17.txt (#136 of 367), time: 19:16:48\n",
      "Importing file: hashtag_file29_folder21to28_14Oct17.txt (#137 of 367), time: 19:16:48\n",
      "Importing file: hashtag_file2_folder11to15_14Oct17.txt (#138 of 367), time: 19:16:49\n",
      "Importing file: hashtag_file2_folder16to20_14Oct17.txt (#139 of 367), time: 19:16:50\n",
      "Importing file: hashtag_file2_folder1to10_14Oct17.txt (#140 of 367), time: 19:16:50\n",
      "Importing file: hashtag_file2_folder21to28_14Oct17.txt (#141 of 367), time: 19:16:51\n",
      "Importing file: hashtag_file30_folder11to15_14Oct17.txt (#142 of 367), time: 19:16:51\n",
      "Importing file: hashtag_file30_folder16to20_14Oct17.txt (#143 of 367), time: 19:16:52\n",
      "Importing file: hashtag_file30_folder1to10_14Oct17.txt (#144 of 367), time: 19:16:53\n",
      "Importing file: hashtag_file30_folder21to28_14Oct17.txt (#145 of 367), time: 19:16:53\n",
      "Importing file: hashtag_file31_folder11to15_14Oct17.txt (#146 of 367), time: 19:16:54\n",
      "Importing file: hashtag_file31_folder16to20_14Oct17.txt (#147 of 367), time: 19:16:54\n",
      "Importing file: hashtag_file31_folder1to10_14Oct17.txt (#148 of 367), time: 19:16:55\n",
      "Importing file: hashtag_file31_folder21to28_14Oct17.txt (#149 of 367), time: 19:16:56\n",
      "Importing file: hashtag_file32_folder11to15_14Oct17.txt (#150 of 367), time: 19:16:56\n",
      "Importing file: hashtag_file32_folder16to20_14Oct17.txt (#151 of 367), time: 19:16:57\n",
      "Importing file: hashtag_file32_folder1to10_14Oct17.txt (#152 of 367), time: 19:16:58\n",
      "Importing file: hashtag_file32_folder21to28_14Oct17.txt (#153 of 367), time: 19:16:58\n",
      "Importing file: hashtag_file33_folder11to15_14Oct17.txt (#154 of 367), time: 19:16:59\n",
      "Importing file: hashtag_file33_folder16to20_14Oct17.txt (#155 of 367), time: 19:17:0\n",
      "Importing file: hashtag_file33_folder1to10_14Oct17.txt (#156 of 367), time: 19:17:0\n",
      "Importing file: hashtag_file33_folder21to28_14Oct17.txt (#157 of 367), time: 19:17:1\n",
      "Importing file: hashtag_file34_folder11to15_14Oct17.txt (#158 of 367), time: 19:17:1\n",
      "Importing file: hashtag_file34_folder16to20_14Oct17.txt (#159 of 367), time: 19:17:2\n",
      "Importing file: hashtag_file34_folder1to10_14Oct17.txt (#160 of 367), time: 19:17:3\n",
      "Importing file: hashtag_file34_folder21to28_14Oct17.txt (#161 of 367), time: 19:17:3\n",
      "Importing file: hashtag_file35_folder11to15_14Oct17.txt (#162 of 367), time: 19:17:4\n",
      "Importing file: hashtag_file35_folder16to20_14Oct17.txt (#163 of 367), time: 19:17:5\n",
      "Importing file: hashtag_file35_folder1to10_14Oct17.txt (#164 of 367), time: 19:17:5\n",
      "Importing file: hashtag_file35_folder21to28_14Oct17.txt (#165 of 367), time: 19:17:6\n",
      "Importing file: hashtag_file36_folder11to15_14Oct17.txt (#166 of 367), time: 19:17:6\n",
      "Importing file: hashtag_file36_folder16to20_14Oct17.txt (#167 of 367), time: 19:17:7\n",
      "Importing file: hashtag_file36_folder1to10_14Oct17.txt (#168 of 367), time: 19:17:8\n",
      "Importing file: hashtag_file36_folder21to28_14Oct17.txt (#169 of 367), time: 19:17:8\n",
      "Importing file: hashtag_file37_folder11to15_14Oct17.txt (#170 of 367), time: 19:17:9\n",
      "Importing file: hashtag_file37_folder16to20_14Oct17.txt (#171 of 367), time: 19:17:10\n",
      "Importing file: hashtag_file37_folder1to10_14Oct17.txt (#172 of 367), time: 19:17:10\n",
      "Importing file: hashtag_file37_folder21to28_14Oct17.txt (#173 of 367), time: 19:17:11\n",
      "Importing file: hashtag_file38_folder11to15_14Oct17.txt (#174 of 367), time: 19:17:12\n",
      "Importing file: hashtag_file38_folder16to20_14Oct17.txt (#175 of 367), time: 19:17:12\n",
      "Importing file: hashtag_file38_folder1to10_14Oct17.txt (#176 of 367), time: 19:17:13\n",
      "Importing file: hashtag_file38_folder21to28_14Oct17.txt (#177 of 367), time: 19:17:14\n",
      "Importing file: hashtag_file39_folder11to15_14Oct17.txt (#178 of 367), time: 19:17:14\n",
      "Importing file: hashtag_file39_folder16to20_14Oct17.txt (#179 of 367), time: 19:17:15\n",
      "Importing file: hashtag_file39_folder1to10_14Oct17.txt (#180 of 367), time: 19:17:16\n",
      "Importing file: hashtag_file39_folder21to28_14Oct17.txt (#181 of 367), time: 19:17:16\n",
      "Importing file: hashtag_file3_folder11to15_14Oct17.txt (#182 of 367), time: 19:17:17\n",
      "Importing file: hashtag_file3_folder16to20_14Oct17.txt (#183 of 367), time: 19:17:18\n",
      "Importing file: hashtag_file3_folder1to10_14Oct17.txt (#184 of 367), time: 19:17:19\n",
      "Importing file: hashtag_file3_folder21to28_14Oct17.txt (#185 of 367), time: 19:17:19\n",
      "Importing file: hashtag_file40_folder11to15_14Oct17.txt (#186 of 367), time: 19:17:20\n",
      "Importing file: hashtag_file40_folder16to20_14Oct17.txt (#187 of 367), time: 19:17:21\n",
      "Importing file: hashtag_file40_folder1to10_14Oct17.txt (#188 of 367), time: 19:17:22\n",
      "Importing file: hashtag_file40_folder21to28_14Oct17.txt (#189 of 367), time: 19:17:22\n",
      "Importing file: hashtag_file41_folder11to15_14Oct17.txt (#190 of 367), time: 19:17:23\n",
      "Importing file: hashtag_file41_folder16to20_14Oct17.txt (#191 of 367), time: 19:17:24\n",
      "Importing file: hashtag_file41_folder1to10_14Oct17.txt (#192 of 367), time: 19:17:24\n",
      "Importing file: hashtag_file41_folder21to28_14Oct17.txt (#193 of 367), time: 19:17:25\n",
      "Importing file: hashtag_file42_folder11to15_14Oct17.txt (#194 of 367), time: 19:17:26\n",
      "Importing file: hashtag_file42_folder16to20_14Oct17.txt (#195 of 367), time: 19:17:27\n",
      "Importing file: hashtag_file42_folder1to10_14Oct17.txt (#196 of 367), time: 19:17:27\n",
      "Importing file: hashtag_file42_folder21to28_14Oct17.txt (#197 of 367), time: 19:17:28\n",
      "Importing file: hashtag_file43_folder11to15_14Oct17.txt (#198 of 367), time: 19:17:29\n",
      "Importing file: hashtag_file43_folder16to20_14Oct17.txt (#199 of 367), time: 19:17:29\n",
      "Importing file: hashtag_file43_folder1to10_14Oct17.txt (#200 of 367), time: 19:17:30\n",
      "Importing file: hashtag_file43_folder21to28_14Oct17.txt (#201 of 367), time: 19:17:31\n",
      "Importing file: hashtag_file44_folder11to15_14Oct17.txt (#202 of 367), time: 19:17:32\n",
      "Importing file: hashtag_file44_folder16to20_14Oct17.txt (#203 of 367), time: 19:17:33\n",
      "Importing file: hashtag_file44_folder1to10_14Oct17.txt (#204 of 367), time: 19:17:33\n",
      "Importing file: hashtag_file44_folder21to28_14Oct17.txt (#205 of 367), time: 19:17:34\n",
      "Importing file: hashtag_file45_folder11to15_14Oct17.txt (#206 of 367), time: 19:17:35\n",
      "Importing file: hashtag_file45_folder16to20_14Oct17.txt (#207 of 367), time: 19:17:36\n",
      "Importing file: hashtag_file45_folder1to10_14Oct17.txt (#208 of 367), time: 19:17:36\n",
      "Importing file: hashtag_file45_folder21to28_14Oct17.txt (#209 of 367), time: 19:17:37\n",
      "Importing file: hashtag_file46_folder11to15_14Oct17.txt (#210 of 367), time: 19:17:38\n",
      "Importing file: hashtag_file46_folder16to20_14Oct17.txt (#211 of 367), time: 19:17:38\n",
      "Importing file: hashtag_file46_folder1to10_14Oct17.txt (#212 of 367), time: 19:17:39\n",
      "Importing file: hashtag_file46_folder21to28_14Oct17.txt (#213 of 367), time: 19:17:40\n",
      "Importing file: hashtag_file47_folder11to15_14Oct17.txt (#214 of 367), time: 19:17:40\n",
      "Importing file: hashtag_file47_folder16to20_14Oct17.txt (#215 of 367), time: 19:17:41\n",
      "Importing file: hashtag_file47_folder1to10_14Oct17.txt (#216 of 367), time: 19:17:42\n",
      "Importing file: hashtag_file47_folder21to28_14Oct17.txt (#217 of 367), time: 19:17:42\n",
      "Importing file: hashtag_file48_folder11to15_14Oct17.txt (#218 of 367), time: 19:17:43\n",
      "Importing file: hashtag_file48_folder16to20_14Oct17.txt (#219 of 367), time: 19:17:44\n",
      "Importing file: hashtag_file48_folder1to10_14Oct17.txt (#220 of 367), time: 19:17:44\n",
      "Importing file: hashtag_file48_folder21to28_14Oct17.txt (#221 of 367), time: 19:17:45\n",
      "Importing file: hashtag_file49_folder11to15_14Oct17.txt (#222 of 367), time: 19:17:46\n",
      "Importing file: hashtag_file49_folder16to20_14Oct17.txt (#223 of 367), time: 19:17:46\n",
      "Importing file: hashtag_file49_folder1to10_14Oct17.txt (#224 of 367), time: 19:17:47\n",
      "Importing file: hashtag_file49_folder21to28_14Oct17.txt (#225 of 367), time: 19:17:48\n",
      "Importing file: hashtag_file4_folder11to15_14Oct17.txt (#226 of 367), time: 19:17:49\n",
      "Importing file: hashtag_file4_folder16to20_14Oct17.txt (#227 of 367), time: 19:17:49\n",
      "Importing file: hashtag_file4_folder1to10_14Oct17.txt (#228 of 367), time: 19:17:50\n",
      "Importing file: hashtag_file4_folder21to28_14Oct17.txt (#229 of 367), time: 19:17:51\n",
      "Importing file: hashtag_file50_folder16to20_14Oct17.txt (#230 of 367), time: 19:17:51\n",
      "Importing file: hashtag_file50_folder1to10_14Oct17.txt (#231 of 367), time: 19:17:52\n",
      "Importing file: hashtag_file50_folder21to28_14Oct17.txt (#232 of 367), time: 19:17:53\n",
      "Importing file: hashtag_file51_folder16to20_14Oct17.txt (#233 of 367), time: 19:17:53\n",
      "Importing file: hashtag_file51_folder1to10_14Oct17.txt (#234 of 367), time: 19:17:54\n",
      "Importing file: hashtag_file51_folder21to28_14Oct17.txt (#235 of 367), time: 19:17:55\n",
      "Importing file: hashtag_file52_folder16to20_14Oct17.txt (#236 of 367), time: 19:17:55\n",
      "Importing file: hashtag_file52_folder1to10_14Oct17.txt (#237 of 367), time: 19:17:56\n",
      "Importing file: hashtag_file52_folder21to28_14Oct17.txt (#238 of 367), time: 19:17:57\n",
      "Importing file: hashtag_file53_folder16to20_14Oct17.txt (#239 of 367), time: 19:17:58\n",
      "Importing file: hashtag_file53_folder1to10_14Oct17.txt (#240 of 367), time: 19:17:58\n",
      "Importing file: hashtag_file53_folder21to28_14Oct17.txt (#241 of 367), time: 19:17:59\n",
      "Importing file: hashtag_file54_folder16to20_14Oct17.txt (#242 of 367), time: 19:18:0\n",
      "Importing file: hashtag_file54_folder1to10_14Oct17.txt (#243 of 367), time: 19:18:0\n",
      "Importing file: hashtag_file54_folder21to28_14Oct17.txt (#244 of 367), time: 19:18:1\n",
      "Importing file: hashtag_file55_folder16to20_14Oct17.txt (#245 of 367), time: 19:18:2\n",
      "Importing file: hashtag_file55_folder1to10_14Oct17.txt (#246 of 367), time: 19:18:3\n",
      "Importing file: hashtag_file55_folder21to28_14Oct17.txt (#247 of 367), time: 19:18:3\n",
      "Importing file: hashtag_file56_folder16to20_14Oct17.txt (#248 of 367), time: 19:18:4\n",
      "Importing file: hashtag_file56_folder1to10_14Oct17.txt (#249 of 367), time: 19:18:5\n",
      "Importing file: hashtag_file56_folder21to28_14Oct17.txt (#250 of 367), time: 19:18:6\n",
      "Importing file: hashtag_file57_folder16to20_14Oct17.txt (#251 of 367), time: 19:18:6\n",
      "Importing file: hashtag_file57_folder1to10_14Oct17.txt (#252 of 367), time: 19:18:7\n",
      "Importing file: hashtag_file57_folder21to28_14Oct17.txt (#253 of 367), time: 19:18:8\n",
      "Importing file: hashtag_file58_folder16to20_14Oct17.txt (#254 of 367), time: 19:18:9\n",
      "Importing file: hashtag_file58_folder1to10_14Oct17.txt (#255 of 367), time: 19:18:9\n",
      "Importing file: hashtag_file58_folder21to28_14Oct17.txt (#256 of 367), time: 19:18:10\n",
      "Importing file: hashtag_file59_folder16to20_14Oct17.txt (#257 of 367), time: 19:18:11\n",
      "Importing file: hashtag_file59_folder1to10_14Oct17.txt (#258 of 367), time: 19:18:12\n",
      "Importing file: hashtag_file59_folder21to28_14Oct17.txt (#259 of 367), time: 19:18:12\n",
      "Importing file: hashtag_file5_folder11to15_14Oct17.txt (#260 of 367), time: 19:18:13\n",
      "Importing file: hashtag_file5_folder16to20_14Oct17.txt (#261 of 367), time: 19:18:14\n",
      "Importing file: hashtag_file5_folder1to10_14Oct17.txt (#262 of 367), time: 19:18:15\n",
      "Importing file: hashtag_file5_folder21to28_14Oct17.txt (#263 of 367), time: 19:18:15\n",
      "Importing file: hashtag_file60_folder16to20_14Oct17.txt (#264 of 367), time: 19:18:16\n",
      "Importing file: hashtag_file60_folder1to10_14Oct17.txt (#265 of 367), time: 19:18:17\n",
      "Importing file: hashtag_file60_folder21to28_14Oct17.txt (#266 of 367), time: 19:18:18\n",
      "Importing file: hashtag_file61_folder16to20_14Oct17.txt (#267 of 367), time: 19:18:18\n",
      "Importing file: hashtag_file61_folder1to10_14Oct17.txt (#268 of 367), time: 19:18:19\n",
      "Importing file: hashtag_file61_folder21to28_14Oct17.txt (#269 of 367), time: 19:18:20\n",
      "Importing file: hashtag_file62_folder16to20_14Oct17.txt (#270 of 367), time: 19:18:21\n",
      "Importing file: hashtag_file62_folder1to10_14Oct17.txt (#271 of 367), time: 19:18:21\n",
      "Importing file: hashtag_file62_folder21to28_14Oct17.txt (#272 of 367), time: 19:18:22\n",
      "Importing file: hashtag_file63_folder16to20_14Oct17.txt (#273 of 367), time: 19:18:23\n",
      "Importing file: hashtag_file63_folder1to10_14Oct17.txt (#274 of 367), time: 19:18:24\n",
      "Importing file: hashtag_file63_folder21to28_14Oct17.txt (#275 of 367), time: 19:18:24\n",
      "Importing file: hashtag_file64_folder16to20_14Oct17.txt (#276 of 367), time: 19:18:25\n",
      "Importing file: hashtag_file64_folder1to10_14Oct17.txt (#277 of 367), time: 19:18:26\n",
      "Importing file: hashtag_file64_folder21to28_14Oct17.txt (#278 of 367), time: 19:18:27\n",
      "Importing file: hashtag_file65_folder16to20_14Oct17.txt (#279 of 367), time: 19:18:27\n",
      "Importing file: hashtag_file65_folder1to10_14Oct17.txt (#280 of 367), time: 19:18:28\n",
      "Importing file: hashtag_file65_folder21to28_14Oct17.txt (#281 of 367), time: 19:18:29\n",
      "Importing file: hashtag_file66_folder16to20_14Oct17.txt (#282 of 367), time: 19:18:30\n",
      "Importing file: hashtag_file66_folder1to10_14Oct17.txt (#283 of 367), time: 19:18:30\n",
      "Importing file: hashtag_file66_folder21to28_14Oct17.txt (#284 of 367), time: 19:18:31\n",
      "Importing file: hashtag_file67_folder1to10_14Oct17.txt (#285 of 367), time: 19:18:32\n",
      "Importing file: hashtag_file67_folder21to28_14Oct17.txt (#286 of 367), time: 19:18:33\n",
      "Importing file: hashtag_file68_folder1to10_14Oct17.txt (#287 of 367), time: 19:18:34\n",
      "Importing file: hashtag_file68_folder21to28_14Oct17.txt (#288 of 367), time: 19:18:34\n",
      "Importing file: hashtag_file69_folder1to10_14Oct17.txt (#289 of 367), time: 19:18:35\n",
      "Importing file: hashtag_file69_folder21to28_14Oct17.txt (#290 of 367), time: 19:18:36\n",
      "Importing file: hashtag_file6_folder11to15_14Oct17.txt (#291 of 367), time: 19:18:37\n",
      "Importing file: hashtag_file6_folder16to20_14Oct17.txt (#292 of 367), time: 19:18:37\n",
      "Importing file: hashtag_file6_folder1to10_14Oct17.txt (#293 of 367), time: 19:18:38\n",
      "Importing file: hashtag_file6_folder21to28_14Oct17.txt (#294 of 367), time: 19:18:39\n",
      "Importing file: hashtag_file70_folder1to10_14Oct17.txt (#295 of 367), time: 19:18:40\n",
      "Importing file: hashtag_file70_folder21to28_14Oct17.txt (#296 of 367), time: 19:18:40\n",
      "Importing file: hashtag_file71_folder1to10_14Oct17.txt (#297 of 367), time: 19:18:41\n",
      "Importing file: hashtag_file71_folder21to28_14Oct17.txt (#298 of 367), time: 19:18:42\n",
      "Importing file: hashtag_file72_folder1to10_14Oct17.txt (#299 of 367), time: 19:18:43\n",
      "Importing file: hashtag_file72_folder21to28_14Oct17.txt (#300 of 367), time: 19:18:43\n",
      "Importing file: hashtag_file73_folder1to10_14Oct17.txt (#301 of 367), time: 19:18:44\n",
      "Importing file: hashtag_file73_folder21to28_14Oct17.txt (#302 of 367), time: 19:18:45\n",
      "Importing file: hashtag_file74_folder1to10_14Oct17.txt (#303 of 367), time: 19:18:46\n",
      "Importing file: hashtag_file74_folder21to28_14Oct17.txt (#304 of 367), time: 19:18:47\n",
      "Importing file: hashtag_file75_folder1to10_14Oct17.txt (#305 of 367), time: 19:18:47\n",
      "Importing file: hashtag_file75_folder21to28_14Oct17.txt (#306 of 367), time: 19:18:48\n",
      "Importing file: hashtag_file76_folder1to10_14Oct17.txt (#307 of 367), time: 19:18:49\n",
      "Importing file: hashtag_file76_folder21to28_14Oct17.txt (#308 of 367), time: 19:18:50\n",
      "Importing file: hashtag_file77_folder1to10_14Oct17.txt (#309 of 367), time: 19:18:51\n",
      "Importing file: hashtag_file77_folder21to28_14Oct17.txt (#310 of 367), time: 19:18:51\n",
      "Importing file: hashtag_file78_folder1to10_14Oct17.txt (#311 of 367), time: 19:18:52\n",
      "Importing file: hashtag_file78_folder21to28_14Oct17.txt (#312 of 367), time: 19:18:53\n",
      "Importing file: hashtag_file79_folder1to10_14Oct17.txt (#313 of 367), time: 19:18:54\n",
      "Importing file: hashtag_file79_folder21to28_14Oct17.txt (#314 of 367), time: 19:18:54\n",
      "Importing file: hashtag_file7_folder11to15_14Oct17.txt (#315 of 367), time: 19:18:55\n",
      "Importing file: hashtag_file7_folder16to20_14Oct17.txt (#316 of 367), time: 19:18:56\n",
      "Importing file: hashtag_file7_folder1to10_14Oct17.txt (#317 of 367), time: 19:18:57\n",
      "Importing file: hashtag_file7_folder21to28_14Oct17.txt (#318 of 367), time: 19:18:58\n",
      "Importing file: hashtag_file80_folder1to10_14Oct17.txt (#319 of 367), time: 19:18:58\n",
      "Importing file: hashtag_file80_folder21to28_14Oct17.txt (#320 of 367), time: 19:18:59\n",
      "Importing file: hashtag_file81_folder1to10_14Oct17.txt (#321 of 367), time: 19:19:0\n",
      "Importing file: hashtag_file81_folder21to28_14Oct17.txt (#322 of 367), time: 19:19:1\n",
      "Importing file: hashtag_file82_folder1to10_14Oct17.txt (#323 of 367), time: 19:19:2\n",
      "Importing file: hashtag_file82_folder21to28_14Oct17.txt (#324 of 367), time: 19:19:2\n",
      "Importing file: hashtag_file83_folder1to10_14Oct17.txt (#325 of 367), time: 19:19:3\n",
      "Importing file: hashtag_file83_folder21to28_14Oct17.txt (#326 of 367), time: 19:19:4\n",
      "Importing file: hashtag_file84_folder1to10_14Oct17.txt (#327 of 367), time: 19:19:5\n",
      "Importing file: hashtag_file84_folder21to28_14Oct17.txt (#328 of 367), time: 19:19:6\n",
      "Importing file: hashtag_file85_folder1to10_14Oct17.txt (#329 of 367), time: 19:19:6\n",
      "Importing file: hashtag_file85_folder21to28_14Oct17.txt (#330 of 367), time: 19:19:7\n",
      "Importing file: hashtag_file86_folder1to10_14Oct17.txt (#331 of 367), time: 19:19:8\n",
      "Importing file: hashtag_file86_folder21to28_14Oct17.txt (#332 of 367), time: 19:19:9\n",
      "Importing file: hashtag_file87_folder1to10_14Oct17.txt (#333 of 367), time: 19:19:10\n",
      "Importing file: hashtag_file87_folder21to28_14Oct17.txt (#334 of 367), time: 19:19:11\n",
      "Importing file: hashtag_file88_folder1to10_14Oct17.txt (#335 of 367), time: 19:19:11\n",
      "Importing file: hashtag_file88_folder21to28_14Oct17.txt (#336 of 367), time: 19:19:12\n",
      "Importing file: hashtag_file89_folder1to10_14Oct17.txt (#337 of 367), time: 19:19:13\n",
      "Importing file: hashtag_file89_folder21to28_14Oct17.txt (#338 of 367), time: 19:19:14\n",
      "Importing file: hashtag_file8_folder11to15_14Oct17.txt (#339 of 367), time: 19:19:15\n",
      "Importing file: hashtag_file8_folder16to20_14Oct17.txt (#340 of 367), time: 19:19:15\n",
      "Importing file: hashtag_file8_folder1to10_14Oct17.txt (#341 of 367), time: 19:19:16\n",
      "Importing file: hashtag_file8_folder21to28_14Oct17.txt (#342 of 367), time: 19:19:17\n",
      "Importing file: hashtag_file90_folder1to10_14Oct17.txt (#343 of 367), time: 19:19:18\n",
      "Importing file: hashtag_file90_folder21to28_14Oct17.txt (#344 of 367), time: 19:19:19\n",
      "Importing file: hashtag_file91_folder1to10_14Oct17.txt (#345 of 367), time: 19:19:19\n",
      "Importing file: hashtag_file91_folder21to28_14Oct17.txt (#346 of 367), time: 19:19:20\n",
      "Importing file: hashtag_file92_folder1to10_14Oct17.txt (#347 of 367), time: 19:19:21\n",
      "Importing file: hashtag_file92_folder21to28_14Oct17.txt (#348 of 367), time: 19:19:22\n",
      "Importing file: hashtag_file93_folder1to10_14Oct17.txt (#349 of 367), time: 19:19:23\n",
      "Importing file: hashtag_file93_folder21to28_14Oct17.txt (#350 of 367), time: 19:19:24\n",
      "Importing file: hashtag_file94_folder1to10_14Oct17.txt (#351 of 367), time: 19:19:24\n",
      "Importing file: hashtag_file94_folder21to28_14Oct17.txt (#352 of 367), time: 19:19:25\n",
      "Importing file: hashtag_file95_folder1to10_14Oct17.txt (#353 of 367), time: 19:19:26\n",
      "Importing file: hashtag_file95_folder21to28_14Oct17.txt (#354 of 367), time: 19:19:27\n",
      "Importing file: hashtag_file96_folder1to10_14Oct17.txt (#355 of 367), time: 19:19:28\n",
      "Importing file: hashtag_file96_folder21to28_14Oct17.txt (#356 of 367), time: 19:19:29\n",
      "Importing file: hashtag_file97_folder1to10_14Oct17.txt (#357 of 367), time: 19:19:29\n",
      "Importing file: hashtag_file97_folder21to28_14Oct17.txt (#358 of 367), time: 19:19:30\n",
      "Importing file: hashtag_file98_folder1to10_14Oct17.txt (#359 of 367), time: 19:19:31\n",
      "Importing file: hashtag_file98_folder21to28_14Oct17.txt (#360 of 367), time: 19:19:32\n",
      "Importing file: hashtag_file99_folder1to10_14Oct17.txt (#361 of 367), time: 19:19:33\n",
      "Importing file: hashtag_file99_folder21to28_14Oct17.txt (#362 of 367), time: 19:19:33\n",
      "Importing file: hashtag_file9_folder11to15_14Oct17.txt (#363 of 367), time: 19:19:34\n",
      "Importing file: hashtag_file9_folder16to20_14Oct17.txt (#364 of 367), time: 19:19:35\n",
      "Importing file: hashtag_file9_folder1to10_14Oct17.txt (#365 of 367), time: 19:19:36\n",
      "Importing file: hashtag_file9_folder21to28_14Oct17.txt (#366 of 367), time: 19:19:37\n",
      "\n",
      "Done! End time: 19:19:38\n"
     ]
    }
   ],
   "source": [
    "input_path = './hashtag_tweets/'\n",
    "\n",
    "num_files = np.inf\n",
    "hashtags = ['sarcasm','sarcastic','irony','ironic','happy','sad','seriously']\n",
    "\n",
    "\"\"\"Check whether hashtag dataframes already exist\"\"\"\n",
    "df_list = ['tweets_all', 'tweets_sarcasm', 'tweets_sad', 'tweets_happy', 'tweets_seriously']\n",
    "\n",
    "for df in df_list:\n",
    "    filepath = './hashtag_dfs/' + df + '.txt'\n",
    "    if not os.path.isfile(filepath):        \n",
    "        dfs_exist = False\n",
    "    else:\n",
    "        dfs_exist = True\n",
    "\n",
    "if dfs_exist == False:\n",
    "    \"\"\"if hashtag dataframes DON'T already exist, get input and create them\"\"\"\n",
    "    \n",
    "    print('Dataframes do not exist, creating now.')\n",
    "    tweets_all = get_input(input_path, num_files, hashtags)\n",
    "else:\n",
    "    \"\"\"if hashtag dataframes DO already exist, load them\"\"\"\n",
    "\n",
    "    print('Dataframes already exist, loading now.')\n",
    "    \n",
    "    dict_dfs = read_in_dfs(df_list, hashtags)\n",
    "    tweets_all = dict_dfs['tweets_all']    \n",
    "    tweets_sarcasm = dict_dfs['tweets_sarcasm']    \n",
    "    tweets_happy = dict_dfs['tweets_happy']    \n",
    "    tweets_sad = dict_dfs['tweets_sad']    \n",
    "    tweets_seriously = dict_dfs['tweets_seriously']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:21:15.713927Z",
     "start_time": "2017-10-16T19:20:37.870111-04:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Rachel/.pyenv/versions/py36_env/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"If dataframes do NOT exist, create them and save them as text files\"\"\"\n",
    "if dfs_exist == False:\n",
    "    \n",
    "    # subset data to tweets with only one hashtag\n",
    "    tweets_all = count_total_ht(tweets_all)\n",
    "    tweets_all = tweets_all[tweets_all.num_ht == 1]\n",
    "    \n",
    "    # remove duplicate tweets & drop rows with no tweet text\n",
    "    tweets_all = clean_data(tweets_all)\n",
    "    \n",
    "    # create subset dataframes\n",
    "    tweets_sarcasm = tweets_all[tweets_all.sarcasm == 1]\n",
    "    tweets_happy = tweets_all[tweets_all.happy == 1]\n",
    "    tweets_sad = tweets_all[tweets_all.sad == 1]\n",
    "    tweets_seriously = tweets_all[tweets_all.seriously == 1]\n",
    "    \n",
    "    # only include tweets with hashtag at end \n",
    "    pattern = r'\\bsarcasm\\b$'\n",
    "    tweets_sarcasm = tweets_sarcasm[(tweets_sarcasm['text'].str.contains(pattern, case=False, na=False, regex=True))]\n",
    "\n",
    "    pattern = r'\\bhappy\\b$'\n",
    "    tweets_happy = tweets_happy[(tweets_happy['text'].str.contains(pattern, case=False, na=False, regex=True))]\n",
    "\n",
    "    pattern = r'\\bsad\\b$'\n",
    "    tweets_sad = tweets_sad[(tweets_sad['text'].str.contains(pattern, case=False, na=False, regex=True))]\n",
    "\n",
    "    pattern = r'\\bseriously\\b$'\n",
    "    tweets_seriously = tweets_seriously[(tweets_seriously['text'].str.contains(pattern, case=False, na=False, regex=True))]\n",
    "    \n",
    "    # remove hashtag from tweet text\n",
    "    tweets_sarcasm['text'] = tweets_sarcasm['text'].str.replace('#sarcasm', '', case=False)\n",
    "    tweets_happy['text'] = tweets_happy['text'].str.replace('#happy', '', case=False)\n",
    "    tweets_sad['text'] = tweets_sad['text'].str.replace('#sad', '', case=False)\n",
    "    tweets_seriously['text'] = tweets_seriously['text'].str.replace('#seriously', '', case=False)\n",
    "    \n",
    "    # create new dataframe containing all tweets for above hashtags only\n",
    "    tweets_all = pd.concat([tweets_sarcasm, tweets_happy, tweets_sad, tweets_seriously])\n",
    "    \n",
    "    # save dataframes\n",
    "    filename = 'tweets_all'\n",
    "    filepath = './hashtag_dfs/' + filename + '.txt'\n",
    "\n",
    "    # Save df if file doesn't exist\n",
    "    tweets_all.to_csv(filepath, sep = '\\t', index = True, header = True)\n",
    "\n",
    "    filename = 'tweets_sarcasm'\n",
    "    filepath = './hashtag_dfs/' + filename + '.txt'\n",
    "\n",
    "    # Save df if file doesn't exist\n",
    "    tweets_sarcasm.to_csv(filepath, sep = '\\t', index = True, header = True)\n",
    "\n",
    "    filename = 'tweets_happy'\n",
    "    filepath = './hashtag_dfs/' + filename + '.txt'\n",
    "\n",
    "    # Save df if file doesn't exist\n",
    "    tweets_happy.to_csv(filepath, sep = '\\t', index = True, header = True)\n",
    "\n",
    "    filename = 'tweets_sad'\n",
    "    filepath = './hashtag_dfs/' + filename + '.txt'\n",
    "\n",
    "    # Save df if file doesn't exist\n",
    "    tweets_sad.to_csv(filepath, sep = '\\t', index = True, header = True)\n",
    "\n",
    "    filename = 'tweets_seriously'\n",
    "    filepath = './hashtag_dfs/' + filename + '.txt'\n",
    "\n",
    "    # Save df if file doesn't exist\n",
    "    tweets_seriously.to_csv(filepath, sep = '\\t', index = True, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of tweets and users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:24:13.280059Z",
     "start_time": "2017-10-16T19:24:13.250153-04:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of tweets with sarcasm hashtag: 30,910\n",
      "Number of users with sarcasm hashtag: 23,509\n",
      "\n",
      "Number of tweets with happy hashtag: 12,639\n",
      "Number of users with happy hashtag: 10,149\n",
      "\n",
      "Number of tweets with sad hashtag: 40,861\n",
      "Number of users with sad hashtag: 26,456\n",
      "\n",
      "Number of tweets with seriously hashtag: 11,450\n",
      "Number of users with seriously hashtag: 9,033\n"
     ]
    }
   ],
   "source": [
    "count_ht(tweets_sarcasm, False, 'sarcasm')\n",
    "count_ht(tweets_happy, False, 'happy')\n",
    "count_ht(tweets_sad, False, 'sad')\n",
    "count_ht(tweets_seriously, False, 'seriously')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create subset dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:24:31.353935Z",
     "start_time": "2017-10-16T19:24:31.307301-04:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subset tweets_all to only include happy, sad, and sarcasm\n",
    "tweets_all2 = tweets_all[(tweets_all.sarcasm == 1) |\n",
    "                         (tweets_all.happy == 1)]\n",
    "\n",
    "# remove blanks\n",
    "tweets_all2 = tweets_all2[pd.notnull(tweets_all2['text'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:25:20.388861Z",
     "start_time": "2017-10-16T19:25:20.181000-04:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and test set files do not exist, generating now.\n",
      "Saving mask to ./training_test_sets/mask_sarcasm_happy2.txt\n",
      "\n",
      "Length of test: 13,065\n",
      "Length of train: 30,484\n",
      "\n",
      "Percent of sarcasm in training set: 70.85%\n",
      "Percent of sarcasm in test set: 71.27%\n"
     ]
    }
   ],
   "source": [
    "filename_base = 'sarcasm_happy2'\n",
    "strat = True\n",
    "train, test, mask = create_train_test(tweets_all2, filename_base)\n",
    "\n",
    "prop_sarc = sum(train['sarcasm'])/len(train)\n",
    "print('\\nPercent of sarcasm in training set: {:.2f}%'.format(prop_sarc*100))\n",
    "\n",
    "prop_sarc = sum(test['sarcasm'])/len(test)\n",
    "print('Percent of sarcasm in test set: {:.2f}%'.format(prop_sarc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:44:30.029612Z",
     "start_time": "2017-10-16T19:40:12.595985-04:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8548, alpha: 1, min_df: 5, max_df: 0.8\n"
     ]
    }
   ],
   "source": [
    "alphas = [.001, .01, .1, 1, 5]\n",
    "min_dfs = [1, 2, 5, 10, 15, 20]\n",
    "max_dfs = [.6, .7, .8, .9, 1.0]\n",
    "score_func = auc # options: log_likelihood, cohenk, auc\n",
    "\n",
    "best_alpha, best_min_df, best_max_df, clf = perform_cv(tweets_all2, mask, alphas, min_dfs, max_dfs, score_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:48:29.433193Z",
     "start_time": "2017-10-16T19:44:56.045674-04:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.52, alpha: 1, min_df: 5, max_df: 0.7\n"
     ]
    }
   ],
   "source": [
    "alphas = [.001, .01, .1, 1, 5]\n",
    "min_dfs = [1, 2, 5, 10, 15, 20]\n",
    "max_dfs = [.6, .7, .8, .9, 1.0]\n",
    "score_func = cohenk # options: log_likelihood, cohenk, auc\n",
    "\n",
    "best_alpha, best_min_df, best_max_df, clf = perform_cv(tweets_all2, mask, alphas, min_dfs, max_dfs, score_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:52:06.625943Z",
     "start_time": "2017-10-16T19:48:37.955164-04:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -2.695e+03, alpha: 5, min_df: 10, max_df: 0.7\n"
     ]
    }
   ],
   "source": [
    "alphas = [.001, .01, .1, 1, 5]\n",
    "min_dfs = [1, 2, 5, 10, 15, 20]\n",
    "max_dfs = [.6, .7, .8, .9, 1.0]\n",
    "score_func = log_likelihood # options: log_likelihood, cohenk, auc\n",
    "\n",
    "best_alpha, best_min_df, best_max_df, clf = perform_cv(tweets_all2, mask, alphas, min_dfs, max_dfs, score_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-16T23:56:00.377393Z",
     "start_time": "2017-10-16T19:55:58.336758-04:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total # of words in vocabulary: 3,855\n",
      "Most frequent words\n",
      "\n",
      " 1430       time\n",
      " 1465       know\n",
      " 1693      today\n",
      " 2130      great\n",
      " 2219       like\n",
      " 2469       good\n",
      " 2473        day\n",
      " 2599       love\n",
      " 3099       just\n",
      " 4485      quote\n",
      "\n",
      "Total # of words in vocabulary: 3,855\n",
      "Least frequent words\n",
      "\n",
      "   10     makeup\n",
      "   10       rage\n",
      "   10    focused\n",
      "   10      verge\n",
      "   10 appointments\n",
      "   10    puppies\n",
      "   10      squad\n",
      "   10 foxandfriends\n",
      "   10      fraud\n",
      "   10   conflict\n",
      "\n",
      "Most predictive words\n",
      "\tP(sarcasm | word)\n",
      "realdonaldtrump 0.97\n",
      "          trump 0.97\n",
      "          women 0.95\n",
      "           yeah 0.95\n",
      "            isn 0.95\n",
      "        clearly 0.94\n",
      "             oh 0.94\n",
      "           aren 0.94\n",
      "           poor 0.94\n",
      "      obviously 0.94\n",
      "\n",
      "Least predictive words\n",
      "\tP(sarcasm | word)\n",
      "           soul 0.31\n",
      "           mood 0.30\n",
      "           cake 0.29\n",
      "           wine 0.29\n",
      "          dance 0.28\n",
      "        smiling 0.27\n",
      "       gorgeous 0.26\n",
      "      happiness 0.19\n",
      "          smile 0.17\n",
      "       pharrell 0.11\n",
      "\n",
      "Accuracy for training set = 83.16%\n",
      "Accuracy for test set = 79.25%\n",
      "Cohen's Kappa = 0.45\n",
      "Precision = 0.81\n",
      "AUC = 0.83\n",
      "True positive rate = 0.48\n",
      "True negative rate = 0.92\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAEmCAYAAAAnRIjxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13a8e9668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=best_min_df, max_df=best_max_df, stop_words = 'english')       \n",
    "\n",
    "X, y, _ = make_xy(tweets_all2, vectorizer)\n",
    "\n",
    "freq_words(10, vectorizer, X, True)\n",
    "freq_words(10, vectorizer, X, False)\n",
    "\n",
    "Xtrain = X[mask]\n",
    "ytrain = y[mask]\n",
    "Xtest = X[~mask]\n",
    "ytest = y[~mask]\n",
    "\n",
    "clf = MultinomialNB(alpha=best_alpha).fit(Xtrain, ytrain)\n",
    "\n",
    "predictive_words(10, vectorizer, clf, Xtest)\n",
    "\n",
    "model_eval(clf, Xtrain, Xtest, ytrain, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
